{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ESA2kDcMmNkJ"
   },
   "source": [
    "# Assignment 3 - Sequence Tagging\n",
    "\n",
    "In this assignment we are going to develop two models for sequence tagging (labeling). As we learned in class, in sequence tagging we have an input sequence X = x1 ... xn and the goal is to assign a label (tag) yi to each word xi in the sequence.\n",
    "\n",
    "We will develop (1) a Hidden Markov Model (HMM) and (2) an RNN-based sequence tagger. We will also implement decoding with the Viterbi algorithm, since as we saw in class, greedily assigning the most likely tag for each token can result in a suboptimal sequence of POS tags.\n",
    "\n",
    "![](https://drive.google.com/uc?export=view&id=1HdBAFpMMEXQxhdLJfs4eAPZNXv_47ngz)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9AD7kg1fkklb"
   },
   "source": [
    "### Loading the Dataset\n",
    "\n",
    "If you're working in Colab, make sure you upload the file `corpus.txt` to the working directory.\n",
    "\n",
    "We will load and save the `corpus.txt` POS-annotated corpus in a format different from its origianl one. In the file, each word is followed by an underscore and a tag that represents the word's correct part of speech in that context. For instance:\n",
    "\n",
    "<br>\n",
    "\n",
    "*(1) There_EX are_VBP also_RB plant_NN and_CC gift_NN shops_NNS ._.*\n",
    "\n",
    "*(2) Tosco_NNP said_VBD it_PRP expects_VBZ BP_NNP to_TO shut_VB the_DT plant_NN ._.*\n",
    "\n",
    "<br>\n",
    "\n",
    "We will instead reformat and load the same info as (token, pos_tag) lists. For example:\n",
    "\n",
    "<br>\n",
    "\n",
    "[\n",
    "  [(there, EX), (are, VBP), (also, RB), (plant, NN), (and, CC), (gift, NN), (shop, NNS), (., .)],\n",
    "\n",
    "  [(tosco, NNP), (said, VBD), (it, PRP), (expects, VBZ), (BP, NNP), (to, To), (shut, VB), (the, DT), (plant, NN), (., .)]\n",
    "]\n",
    "\n",
    "<br>\n",
    "\n",
    "**Please note that we convert each token into its lower case.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "PmreO0EUmLm-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('brainpower', 'NNP'), (',', ','), ('not', 'RB'), ('physical', 'JJ'), ('plant', 'NN'), (',', ','), ('is', 'VBZ'), ('now', 'RB'), ('a', 'DT'), ('firm', 'NN'), (\"'s\", 'POS'), ('chief', 'JJ'), ('asset', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "dataset = [[tuple(w.split('_')) for w in line.strip().split()]\n",
    "           for line in open(\"corpus.txt\")]\n",
    "\n",
    "# Lowercase the words\n",
    "dataset = [[(w.lower(), pos) for (w, pos) in sentence] for sentence in dataset]\n",
    "print(dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Os7EtCwL231C"
   },
   "source": [
    "## Part 1 - Count-based Sequence Tagging with Viterbi Algorithm\n",
    "\n",
    "In the first part of this assignment, we will build a hidden Markov nodel to predict the part of speech tags for the words in a given sentence, using the *Viterbi Algorithm* (see textbook sec 17.4.4). In class we learned about the *Forward Algorithm* and Viterbi is a slight variation of that.\n",
    "\n",
    "![](https://drive.google.com/uc?export=view&id=1Hf0LEE9cvJTbetDcdlzgordreff0u1av)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JHk1JrnzDCgc"
   },
   "source": [
    "We start by obtaining the set of states for the HHM (the POSs) and set of observations (the tokens)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "HWtlNcbf9Ga4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of observations: 7602. ['$', '%', '&', \"'\", \"''\", '...', 'zoological', 'zurich', 'zwetsche', 'zyrtec', '~']\n",
      "Number of states: 40. ['$', \"''\", ',', '-LRB-', '-RRB-', '...', 'WDT', 'WP', 'WP$', 'WRB', '``']\n"
     ]
    }
   ],
   "source": [
    "observations = sorted(list(set.union(*[set(list(zip(*line))[0]) for line in dataset])))\n",
    "example = observations[:5] + [\"...\"] + observations[-5:]\n",
    "print(f\"Number of observations: {len(observations)}. {example}\")\n",
    "\n",
    "states = sorted(list(set.union(*[set(list(zip(*line))[1]) for line in dataset])))\n",
    "example = states[:5] + [\"...\"] + states[-5:]\n",
    "print(f\"Number of states: {len(states)}. {example}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5phXfOnGueR_"
   },
   "source": [
    "We would need to deal with out-of-vocabulary tokens that may appear during inference time (e.g. on a held-out test set). We solve this by adding an unknown token `<unk>` to the vocabulary (i.e., set of observations). In order for this token to get emission probabilities, we would need to add it into the dataset. We do so by replacing every token that appeared less than 2 times in the dataset with the `<unk>` token. Complete the code below to implement this solution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "oW-YJkjcrr7R"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3137\n"
     ]
    }
   ],
   "source": [
    "####################################\n",
    "#   Your code here\n",
    "####################################\n",
    "\n",
    "freq = {}\n",
    "for sentence in dataset:\n",
    "    for word, _ in sentence:\n",
    "        if word in freq:\n",
    "            freq[word] += 1\n",
    "        else:\n",
    "            freq[word] = 1\n",
    "\n",
    "for i, sentence in enumerate(dataset):\n",
    "    new_sentence = []\n",
    "    for word, pos in sentence:\n",
    "        if freq[word] >= 2:\n",
    "            new_sentence.append((word, pos))\n",
    "        else:\n",
    "            new_sentence.append((\"<unk>\", pos))\n",
    "    dataset[i] = new_sentence\n",
    "        \n",
    "####################################\n",
    "\n",
    "# Print <unk> frequency\n",
    "print(sum([1 if w == \"<unk>\" else 0 for instance in dataset for w, _ in instance]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HBqKKgjJzWCT"
   },
   "source": [
    "We are now ready to compute the emission probabilities. The emission probability matrix can be formed as a dictionary where the key is the POS tag, and the value is another dictionary, from a token to a probability. We will use MLE, i.e. the probability is computed as relative frequency, and we will apply add-1 smoothing to avoid 0 probabilities. Complete the code below. Don't forget to add the unobserved words in the smoothed probability distribution for each POS!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "GR8sPEVUxwHm"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Emission Probability Sanity Check ===\n",
      "Number of POS tags (states): 16\n",
      "Sample of tags: ['``', 'WP', 'PRP', 'VBD', ',', 'MD', 'RB', 'VB', 'NN', 'VBG']\n",
      "Emission probability of 'book' in noun: 0.00022366360993066427\n",
      "Emission probability of 'book' in verb: 0.00022386389075442132\n"
     ]
    }
   ],
   "source": [
    "####################################\n",
    "#   Your code here\n",
    "####################################\n",
    "\n",
    "# Declare tag count and emission count dictionaries\n",
    "tag_counts = {} # count(tag)\n",
    "emission_counts = {} # count(tag, word)\n",
    "\n",
    "for word, tag in sentence:\n",
    "    # Tag frequency\n",
    "    if tag in tag_counts:\n",
    "        tag_counts[tag] += 1\n",
    "    else:\n",
    "        tag_counts[tag] = 1\n",
    "\n",
    "    # Word given tag\n",
    "    if tag not in emission_counts:\n",
    "        emission_counts[tag] = {}\n",
    "\n",
    "    # 2D dictionary to count word appearance per tag\n",
    "    if word in emission_counts[tag]:\n",
    "        emission_counts[tag][word] += 1\n",
    "    else:\n",
    "        emission_counts[tag][word] = 1\n",
    "\n",
    "# Collect the vocabulary\n",
    "vocab = set(word for sentence in dataset for word, _ in sentence)\n",
    "V = len(vocab)\n",
    "\n",
    "# Compute emission probabilities\n",
    "emission_probability = {}\n",
    "\n",
    "for tag in emission_counts:\n",
    "    emission_probability[tag] = {}\n",
    "    for word in vocab:\n",
    "        count_word_given_tag = emission_counts[tag].get(word, 0)\n",
    "        emission_probability[tag][word] = (count_word_given_tag + 1) / (tag_counts[tag] + V) # Add-one smoothing\n",
    "\n",
    "####################################\n",
    "\n",
    "print(f\"Emission probability of 'book' in noun: {emission_probability['NN']['book']}\")\n",
    "print(f\"Emission probability of 'book' in verb: {emission_probability['VB']['book']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BrgHDEf23WOO"
   },
   "source": [
    "Now, let's compute the transition probabilities. The transition matrix can be similarly formed as a dictionary, this time from a state (pos) to a dictionary of states to probabilities. Complete the code below to compute the MLE probabilities with Add-1 smoothing. Again, don't forget to include the probabilities from each state to each state (not only those that occurred in the data!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "qRvHUphUz2G7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transition probability from adjective to noun: 0.5382059800664452\n",
      "Transition probability from determiner to verb: 0.2718351324828263\n"
     ]
    }
   ],
   "source": [
    "####################################\n",
    "#   Your code here\n",
    "####################################\n",
    "\n",
    "# Declare tag count and transition count dictionaries\n",
    "transition_counts = {} # count(tag_i-1, tag_i)\n",
    "tag_counts = {} # count(tag_i-1)\n",
    "\n",
    "for sentence in dataset:\n",
    "    # Get tags for sentence\n",
    "    tags = [tag for _, tag in sentence]\n",
    "\n",
    "    for i in range(1, len(tags)):\n",
    "        prev_tag = tags[i-1]\n",
    "        curr_tag = tags[i]\n",
    "\n",
    "        # Count transitions\n",
    "        if prev_tag not in transition_counts:\n",
    "            transition_counts[prev_tag] = {}\n",
    "        if curr_tag not in transition_counts[prev_tag]:\n",
    "            transition_counts[prev_tag][curr_tag] = 0\n",
    "        transition_counts[prev_tag][curr_tag] += 1\n",
    "\n",
    "        # Count occurrences of prev_tag\n",
    "        if prev_tag in tag_counts:\n",
    "            tag_counts[prev_tag] += 1\n",
    "        else:\n",
    "            tag_counts[prev_tag] = 1\n",
    "\n",
    "# Collect all unique tags\n",
    "all_tags = set(tag for sentence in dataset for _, tag in sentence)\n",
    "T = len(all_tags)\n",
    "\n",
    "# Compute transition probabilities\n",
    "transition_probability = {}\n",
    "\n",
    "for prev_tag in all_tags:\n",
    "    transition_probability[prev_tag] = {}\n",
    "    for curr_tag in all_tags:\n",
    "        count_transition = transition_counts.get(prev_tag, {}).get(curr_tag, 0)\n",
    "        count_prev_tag = tag_counts.get(prev_tag, 0)\n",
    "        transition_probability[prev_tag][curr_tag] = (count_transition + 1) / (count_prev_tag + T) # add-one smoothing\n",
    "\n",
    "####################################\n",
    "\n",
    "print(f\"Transition probability from adjective to noun: {transition_probability['JJ']['NN']}\")\n",
    "print(f\"Transition probability from determiner to verb: {transition_probability['VB']['DT']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "etNLMutYUFz9"
   },
   "source": [
    "Finally, we can similarly compute the start probabilities. This will be a dictionary from POS to a probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "nTtle8KIQE5D"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start probability for determiner: 0.2650442477876106\n",
      "Start probability for verb: 0.0017699115044247787\n"
     ]
    }
   ],
   "source": [
    "####################################\n",
    "#   Your code here\n",
    "####################################\n",
    "\n",
    "start_counts = {}\n",
    "\n",
    "for sentence in dataset:\n",
    "    first_tag = sentence[0][1]\n",
    "    if first_tag in start_counts:\n",
    "        start_counts[first_tag] += 1\n",
    "    else:\n",
    "        start_counts[first_tag] = 1\n",
    "\n",
    "# Reusing all_tags and T from previous part\n",
    "num_sentences = len(dataset)\n",
    "\n",
    "# Compute start probabilities with add-one smoothing\n",
    "start_probability = {}\n",
    "for tag in all_tags:\n",
    "    start_probability[tag] = (start_counts.get(tag, 0) + 1) / (num_sentences + T)\n",
    "\n",
    "####################################\n",
    "\n",
    "print(f\"Start probability for determiner: {start_probability['DT']}\")\n",
    "print(f\"Start probability for verb: {start_probability['VB']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hIM4Uyi6U2sX"
   },
   "source": [
    "We're done estimating the probabilities! Now we can implement the decoding method, i.e. find the most probable sequence of POS tags for a given sentence (i.e. sequence of tokens). We will do this by implementing the Viterbi algorithm (see J&M section 8.4.5). Complete the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "FjjacMrG34Hb"
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'$'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 49\u001b[39m\n\u001b[32m     46\u001b[39m     best_path = best_path[::-\u001b[32m1\u001b[39m]\n\u001b[32m     47\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m best_path, best_prob\n\u001b[32m---> \u001b[39m\u001b[32m49\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[43mViterbi\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mI enjoy this assignment\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlower\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     50\u001b[39m \u001b[43m              \u001b[49m\u001b[43mstates\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_probability\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     51\u001b[39m \u001b[43m              \u001b[49m\u001b[43mtransition_probability\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43memission_probability\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 19\u001b[39m, in \u001b[36mViterbi\u001b[39m\u001b[34m(curr_observations, states, start_prob, trans_prob, em_prob)\u001b[39m\n\u001b[32m     16\u001b[39m \u001b[38;5;66;03m# Initialization step: set the path probability to the start probability\u001b[39;00m\n\u001b[32m     17\u001b[39m \u001b[38;5;66;03m# of the first state * emission probability of the first token.\u001b[39;00m\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m states:\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m   viterbi[s][\u001b[32m0\u001b[39m] = start_prob[s] * \u001b[43mem_prob\u001b[49m\u001b[43m[\u001b[49m\u001b[43ms\u001b[49m\u001b[43m]\u001b[49m[curr_observations[\u001b[32m0\u001b[39m]]\n\u001b[32m     21\u001b[39m \u001b[38;5;66;03m# Recursion step: the probability is the maximum probability\u001b[39;00m\n\u001b[32m     22\u001b[39m \u001b[38;5;66;03m# of transitioning from s_other in time step t-1.\u001b[39;00m\n\u001b[32m     24\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m1\u001b[39m, T):\n",
      "\u001b[31mKeyError\u001b[39m: '$'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def Viterbi(curr_observations, # a sequence of tokens to decode\n",
    "            states, start_prob, trans_prob, em_prob):\n",
    "\n",
    "    T = len(curr_observations)\n",
    "\n",
    "    # Create a probability matrix, viterbi, to denote the probability\n",
    "    # to be in state s in time step t\n",
    "    viterbi = {s: [0] * T for s in states}\n",
    "\n",
    "    # Create the backpointer matrix that saves the previous state\n",
    "    backpointer = {s: [0] * T for s in states}\n",
    "\n",
    "    # Initialization step: set the path probability to the start probability\n",
    "    # of the first state * emission probability of the first token.\n",
    "    for s in states:\n",
    "      viterbi[s][0] = start_prob[s] * em_prob[s][curr_observations[0]]\n",
    "\n",
    "    # Recursion step: the probability is the maximum probability\n",
    "    # of transitioning from s_other in time step t-1.\n",
    "    \n",
    "    for t in range(1, T):\n",
    "      for s in states:\n",
    "        ####################################\n",
    "        #   Your code here\n",
    "        ####################################\n",
    "\n",
    "        ####################################\n",
    "\n",
    "        backpointer[s][t] = s_other\n",
    "        viterbi[s][t] = viterbi[s_other][t-1] * trans_prob[s_other][s] * em_prob[s][curr_observations[t]]\n",
    "\n",
    "    # Termination step: find the most likely state at time step t and trace back\n",
    "    # to find the path.\n",
    "    s_end = states[np.argmax([viterbi[s][T-1] for s in states])]\n",
    "    best_prob = viterbi[s_end][T-1]\n",
    "\n",
    "    best_path = [s_end]\n",
    "    for t in range(T-1, 0, -1):\n",
    "      best_path.append(backpointer[best_path[-1]][t])\n",
    "\n",
    "    best_path = best_path[::-1]\n",
    "    return best_path, best_prob\n",
    "            \n",
    "print(Viterbi(\"I enjoy this assignment\".lower().split(),\n",
    "              states, start_probability,\n",
    "              transition_probability, emission_probability))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SyG4lTR4D7Lg"
   },
   "source": [
    "## Part 2 - LSTM-Based POS Tagger\n",
    "\n",
    "In the second part of this assignment, we will use the same corpus to train a bidirectional stacked LSTM-based neural sequence labeling model to predict the part of speech tags of unknown input sentences.\n",
    "\n",
    "The following figure shows an RNN-based sequence tagger:\n",
    "\n",
    "![](https://drive.google.com/uc?export=view&id=1Hf79ROO3UU8BuePuacjhskYJrIq4lvr_)\n",
    "\n",
    "The inputs at each time step are (possibly pre-trained) word embeddings corresponding to the input tokens. They go into the RNN (e.g. LSTM in our assignment) and the outputs from the RNN at each time step represent go into a softmax layer to produce the distribution over the POS tagset for the given input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iFaEQAQgEIew"
   },
   "source": [
    "First, let's import the required packages.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Cti1kgGqD9a4"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import tqdm\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.autograd import Variable\n",
    "from sklearn.metrics import f1_score\n",
    "from torch.nn.utils import clip_grad_norm_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lps3RqC3FAgA"
   },
   "source": [
    "Since we are training a model, we should now split the dataset into train, dev, and test sets, with the common 80%-10%-10% ratio. We start by shuffling the dataset to avoid any possible ordering bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R6Mdvr5WCfZt"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "random.seed(42)\n",
    "\n",
    "random.shuffle(dataset)\n",
    "\n",
    "training_data = dataset[0:math.floor(len(dataset)*0.8)]\n",
    "dev_data = dataset[math.floor(len(dataset)*0.8):math.floor(len(dataset)*0.9)]\n",
    "test_data = dataset[math.floor(len(dataset)*0.9):]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9WzNS6VagmvT"
   },
   "source": [
    "Next, we need to map tokens to indices so that we can learn the embeddings for each token. Similarly, we need to construct a dictionary of POS tags to indices. We will use the `Vocabulary` class from assignment 1. We will also create a `Corpus` class but adapt it to sequence tagging.\n",
    "\n",
    "As in part 1, we also need to deal with **unknown tokens** in testing, for which we will adapt these classes. Complete the code below to replace each (w, p) in the data to their respective IDs, and replace words that appeared fewer than 2 times with the `<unk>` token. Each instance in `corpus.data` should return a tuple with two items: a list of word IDs and a list of tag IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "03a2841MVwN1"
   },
   "outputs": [],
   "source": [
    "from itertools import count\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "class Vocabulary(object):\n",
    "    def __init__(self, add_special_tokens=True):\n",
    "        self.word2idx = defaultdict(count(0).__next__)\n",
    "        self.add_special_tokens = add_special_tokens\n",
    "\n",
    "        # Add special tokens\n",
    "        if add_special_tokens:\n",
    "          _ = self.word2idx['<start>']\n",
    "          _ = self.word2idx['<end>']\n",
    "          _ = self.word2idx['<unk>']\n",
    "\n",
    "    def add_word(self, word):\n",
    "        _ = self.word2idx[word]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.word2idx)\n",
    "\n",
    "    def idx2word(self):\n",
    "        return {i: w for w, i in self.word2idx.items()}\n",
    "\n",
    "\n",
    "class Corpus(object):\n",
    "    def __init__(self, data, n=2, vocab=None, tagset=None):\n",
    "        # Only initialize for the train corpus.\n",
    "        # Then, for the dev and test corpus, use the vocabulary\n",
    "        # from the training set\n",
    "        if vocab is None:\n",
    "          self.vocab = Vocabulary()\n",
    "        else:\n",
    "          self.vocab = vocab\n",
    "\n",
    "        if tagset is None:\n",
    "          self.tagset = Vocabulary(add_special_tokens=False)\n",
    "        else:\n",
    "          self.tagset = tagset\n",
    "\n",
    "        ####################################\n",
    "        #   Your code here\n",
    "        ####################################\n",
    "\n",
    "        ####################################\n",
    "\n",
    "        self.data = data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dr1Zw9r6Xwnt"
   },
   "source": [
    "Now, let's create the `Corpus` object for each of the train, dev, and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E_WvP6qQXv9R"
   },
   "outputs": [],
   "source": [
    "train = Corpus(training_data)\n",
    "dev = Corpus(dev_data, vocab=train.vocab, tagset=train.tagset)\n",
    "test = Corpus(test_data, vocab=train.vocab, tagset=train.tagset)\n",
    "\n",
    "for s, name in zip([train, dev, test], [\"train\", \"dev\", \"test\"]):\n",
    "  print(f\"{name} number of sentences: {len(s.data)}, vocab size: {len(s.vocab)}, tag set size: {len(train.tagset)}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "maHAotm7rtTg"
   },
   "source": [
    "Now let's design the architecture of our bidirectional stacked LSTM-based POS tagger. It should consist of three layers:\n",
    "\n",
    "* [Embedding layer](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html) (`LSTMTagger.embed`): projecting input token IDs into its embedding space.\n",
    "* [(Bi-)LSTM hidden state layer](https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html) (`LSTMTagger.lstm`):\n",
    "* [Output layer](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html) (`LSTMTagger.output`): converting the hidden states to POS predictions.\n",
    "\n",
    "Complete the code below to define the 3 components. Use the `batch_first=True` arguement for the LSTM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DP60pf4ByqPs"
   },
   "outputs": [],
   "source": [
    "class LSTMTagger(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_size, num_layers, tag_size, device):\n",
    "        super(LSTMTagger, self).__init__()\n",
    "        self.device = device\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.input_size = embedding_dim\n",
    "\n",
    "        ####################################\n",
    "        #   Your code here\n",
    "        ####################################\n",
    "\n",
    "        ####################################\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Embed the word IDs. The input to the LSTM should be:\n",
    "        # [batch_size = 1, seq_len, input_size]\n",
    "        x = self.embed(x).view(1, -1, self.input_size)\n",
    "\n",
    "        # Input all words into the LSTM and get their respective outputs\n",
    "        # The LSTM output should be [batch_size, seq_len, hidden_size * 2]\n",
    "        lstm_output = self.lstm(x)[0]\n",
    "\n",
    "        # Predict the tag for each word\n",
    "        outputs = self.output(lstm_output.view(-1, self.hidden_size * 2))\n",
    "        tags_probs = F.log_softmax(outputs, dim=1)\n",
    "\n",
    "        return tags_probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VQwufkRpr432"
   },
   "source": [
    "Let's define the hyper-parameters initialize the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1zpe1I2clIkV"
   },
   "outputs": [],
   "source": [
    "num_layers = 2\n",
    "embed_size = 128\n",
    "intermediate_size = 256\n",
    "vocab_size = len(train.vocab)\n",
    "tag_size = len(train.tagset)\n",
    "\n",
    "num_epochs = 2\n",
    "learning_rate = 0.002\n",
    "device = 0 if torch.cuda.is_available() else 'cpu'\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "model = LSTMTagger(vocab_size, embed_size, intermediate_size, num_layers, tag_size, device)\n",
    "model.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YN2ZI8wEr90x"
   },
   "source": [
    "We can now train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KLkBijlBlJTg"
   },
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    losses = []\n",
    "\n",
    "    for inputs, targets in tqdm(train.data):\n",
    "        inputs = torch.from_numpy(np.array(inputs)).to(device)\n",
    "        targets = torch.from_numpy(np.array(targets)).to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets.reshape(-1))\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        clip_grad_norm_(model.parameters(), 0.5)\n",
    "        optimizer.step()\n",
    "\n",
    "    # Compute accuracy on the validation set\n",
    "    preds = []\n",
    "    targets = []\n",
    "\n",
    "    for inputs, curr_targets in tqdm(dev.data):\n",
    "      inputs = torch.from_numpy(np.array(inputs)).to(device)\n",
    "      outputs = model(inputs)\n",
    "      curr_preds = torch.argmax(outputs, dim=-1)\n",
    "      preds += curr_preds.detach().cpu().numpy().tolist()\n",
    "      targets += curr_targets\n",
    "\n",
    "    f1 = f1_score(targets, preds, average='macro')\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {np.mean(losses):.4f}, F1 score: {f1:5.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SXCXOQx9sFb-"
   },
   "source": [
    "We can now test the trained model on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NGBnD9cuSxgT"
   },
   "outputs": [],
   "source": [
    "preds = []\n",
    "targets = []\n",
    "\n",
    "for inputs, curr_targets in tqdm(test.data):\n",
    "  inputs = torch.from_numpy(np.array(inputs)).to(device)\n",
    "  outputs = model(inputs)\n",
    "  curr_preds = torch.argmax(outputs, dim=-1)\n",
    "  preds += curr_preds.detach().cpu().numpy().tolist()\n",
    "  targets += curr_targets\n",
    "\n",
    "f1 = f1_score(targets, preds, average='macro')\n",
    "print(f\"Test F1 score: {f1:5.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "37a9aRkBaKvq"
   },
   "source": [
    "Finally, let's print a confusion matrix to see which tags are confused with which other tags. Look at the resulting matrix and see whether you can explain these errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e0fTSi5WNUd5"
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "tagset = [train.tagset.idx2word()[i] for i in range(len(train.tagset))]\n",
    "\n",
    "# Compute the confusion matrix\n",
    "cm = confusion_matrix(targets, preds)\n",
    "\n",
    "# Create a heatmap of the confusion matrix\n",
    "sns.heatmap(cm, annot=False, cmap='Blues', fmt='g', xticklabels=tagset, yticklabels=tagset)\n",
    "\n",
    "# Set the axis labels and title\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python (436_env)",
   "language": "python",
   "name": "436_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
